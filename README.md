# Speech Coach

Speech Coach — сервис для анализа качества публичной речи по загружаемому видео.  
Система извлекает аудио из видеофайла, выполняет локальное распознавание речи с помощью Whisper (через библиотеку faster-whisper) и рассчитывает набор количественных метрик:

- темп речи (слов в минуту);
- слова-паразиты (на русском и английском языках);
- паузы и их распределение;
- структура фраз и ритм речи;
- коэффициент говорения (доля времени, в течение которой спикер действительно говорит).

## Функциональные возможности

- Поддержка распространённых видеоформатов (например, MP4, MOV).
- Локальное распознавание речи с использованием Whisper через faster-whisper, без обращения к внешним платным API.
- Анализ темпа речи по «чистому» времени говорения (без учёта тишины и шумовых фрагментов).
- Обнаружение и подсчёт слов-паразитов на русском и английском языках.
- Анализ пауз:
  - выделение пауз между фразами по сегментам распознанной речи;
  - фильтрация шумных участков (аплодисменты, громкий фон) на основе уровня сигнала.
- Анализ структуры фраз и ритма:
  - классификация длины фраз (короткие / сбалансированные / длинные);
  - оценка вариативности длительности фраз (ритмичность речи).
- Формирование структурированных рекомендаций:
  - формальные, шаблонные советы по категориям: темп речи, слова-паразиты, паузы, структура фраз.
- HTTP API на базе FastAPI, удобное для интеграции с будущим фронтендом и внешними системами.

## Используемые технологии

- Python 3.10+ — основной язык разработки.
- GitHub Actions — непрерывная интеграция (проверка кода, автообновление документации).
- pydantic / pydantic-settings — декларативные модели данных и конфигурация через переменные окружения.
- FastAPI — веб-фреймворк для построения HTTP API.
- Uvicorn — ASGI-сервер.
- FFmpeg — извлечение аудиодорожки из видеофайла.
- faster-whisper — локальная реализация Whisper для распознавания речи.


## Whisper и распознавание речи

Для распознавания речи используется модель Whisper в локальном режиме через библиотеку faster-whisper:

- Оригинальный Whisper (OpenAI): https://github.com/openai/whisper  
- Реализация faster-whisper: https://github.com/SYSTRAN/faster-whisper  

Параметры модели задаются через переменные окружения (файл `.env`):

```env
# Путь к бинарю ffmpeg (если ffmpeg доступен в PATH, можно не изменять)
FFMPEG_PATH=ffmpeg

# Размер модели Whisper: tiny, base, small, medium, large-v3
WHISPER_MODEL=small

# Устройство для инференса:
#    cpu  — запуск на процессоре
#    cuda — запуск на GPU (NVIDIA)
WHISPER_DEVICE=cpu

# Тип вычислений:
#    для cpu: int8, int8_float16, float32
#    для cuda: float16, float32 и др.
WHISPER_COMPUTE_TYPE=int8
```

При первом запуске анализа аудио библиотека faster-whisper автоматически загружает выбранную модель и кэширует её локально.

## Установка и запуск

Ниже приведены единые шаги установки и запуска для всех поддерживаемых систем (Linux, macOS, Windows).  
Различаются только команды активации виртуального окружения, что отмечено в комментариях.

```sh
# Клонирование репозитория
git clone https://github.com/YOUR_USERNAME/speech-coach.git
cd speech-coach

# Создание виртуального окружения
python -m venv .venv

# Активация виртуального окружения:
#    Linux / macOS (bash/zsh):
#        source .venv/bin/activate
#    Windows (PowerShell):
#        .\.venv\Scripts\Activate.ps1
#    Windows (cmd.exe):
#        .venv\Scripts\activate

# Установка зависимостей
pip install -r requirements.txt

# Копирование примера файла окружения и, при необходимости, его редактирование
cp .env.example .env    # Windows: copy .env.example .env

# Проверка наличия ffmpeg (при необходимости установить через пакетный менеджер или с сайта проекта)
ffmpeg -version

# Запуск сервера разработки
uvicorn app.main:app --reload
```

Для выхода из виртуального окружения:

```sh
deactivate
```

## Использование API

После запуска сервера документация и интерактивное тестирование API доступны по адресу:

`http://127.0.0.1:8000/docs` (Swagger UI)

### Основной эндпоинт

`POST /api/v1/analyze` — загрузка видеофайла для анализа речи.

Пример запроса с использованием `curl`:

```sh
curl -X POST "http://127.0.0.1:8000/api/v1/analyze" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/video.mp4"
```

В ответе возвращается JSON, содержащий, в частности:

- общую длительность и чистое время говорения;
- темп речи (слов в минуту);
- статистику по словам-паразитам;
- характеристики пауз;
- характеристики фраз;
- структурированные рекомендации по категориям;
- полный текст транскрипта.

## План развития

- Добавление анализа громкости и приблизительной оценки интонации (pitch).
- Сохранение результатов анализа в базе данных (задача + результат).
- Введение статуса задач и асинхронной обработки длительных видео (очередь задач).
- Добавление модульных тестов для основных компонентов аналитики.
- Подготовка Dockerfile и docker-compose для развёртывания.
- Разработка фронтенд-части в этом же репозитории для визуализации результатов и взаимодействия с API.

## Лицензия

Проект распространяется по лицензии MIT. Подробности см. в файле [LICENSE](LICENSE).
